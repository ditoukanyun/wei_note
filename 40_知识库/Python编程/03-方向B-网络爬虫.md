# Pythonç½‘ç»œçˆ¬è™« (Day 61-65)

> æŒæ¡ç½‘ç»œæ•°æ®é‡‡é›†æŠ€æœ¯ï¼Œå­¦ä¼šä½¿ç”¨Pythonçˆ¬å–ç½‘é¡µæ•°æ®

---

## çˆ¬è™«åŸºç¡€

### çˆ¬è™«åˆæ³•æ€§

- âœ… **åˆæ³•**: çˆ¬å–å…¬å¼€æ•°æ®ã€éµå®ˆrobots.txtã€æ§åˆ¶è¯·æ±‚é¢‘ç‡
- âŒ **è¿æ³•**: çˆ¬å–ä¸ªäººéšç§ã€ç ´è§£ä»˜è´¹å†…å®¹ã€é€ æˆæœåŠ¡å™¨è´Ÿæ‹…

### çˆ¬è™«å·¥ä½œæµç¨‹

```
1. å‘é€HTTPè¯·æ±‚ â†’ 2. è·å–å“åº” â†’ 3. è§£ææ•°æ® â†’ 4. å­˜å‚¨æ•°æ®
```

---

## HTTPè¯·æ±‚

### requestsåº“

```python
import requests

# GETè¯·æ±‚
response = requests.get('https://api.github.com')
print(response.status_code)  # 200
print(response.json())       # JSONæ•°æ®

# å¸¦å‚æ•°çš„è¯·æ±‚
params = {'page': 1, 'limit': 10}
response = requests.get('https://api.example.com/items', params=params)

# POSTè¯·æ±‚
data = {'username': 'admin', 'password': '123456'}
response = requests.post('https://api.example.com/login', data=data)

# JSONè¯·æ±‚
json_data = {'name': 'item', 'price': 100}
response = requests.post('https://api.example.com/items', json=json_data)

# æ·»åŠ è¯·æ±‚å¤´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Authorization': 'Bearer token123',
    'Accept': 'application/json'
}
response = requests.get('https://api.example.com/data', headers=headers)

# å¤„ç†Cookies
session = requests.Session()
session.get('https://example.com/login')
response = session.get('https://example.com/profile')

# ä»£ç†è®¾ç½®
proxies = {
    'http': 'http://127.0.0.1:8080',
    'https': 'http://127.0.0.1:8080',
}
response = requests.get('https://example.com', proxies=proxies)

# è¶…æ—¶è®¾ç½®
try:
    response = requests.get('https://example.com', timeout=5)
except requests.Timeout:
    print("è¯·æ±‚è¶…æ—¶")
```

---

## HTMLè§£æ

### BeautifulSoup

```python
from bs4 import BeautifulSoup
import requests

# è·å–é¡µé¢
url = 'https://example.com'
response = requests.get(url)
response.encoding = 'utf-8'

# è§£æHTML
soup = BeautifulSoup(response.text, 'html.parser')

# æŸ¥æ‰¾å…ƒç´ 
# é€šè¿‡æ ‡ç­¾å
title = soup.find('title').text
paragraphs = soup.find_all('p')

# é€šè¿‡CSSé€‰æ‹©å™¨
items = soup.select('.item')           # class="item"
links = soup.select('a[href]')         # æœ‰hrefå±æ€§çš„aæ ‡ç­¾
first_item = soup.select_one('#main')  # id="main"

# é€šè¿‡å±æ€§
images = soup.find_all('img', {'class': 'thumbnail'})
div_with_id = soup.find('div', id='container')

# æå–æ•°æ®
for item in items:
    title = item.find('h2').text.strip()
    link = item.find('a')['href']
    image_url = item.find('img')['src']
    price = item.find('span', class_='price').text
    
    print(f"æ ‡é¢˜: {title}")
    print(f"é“¾æ¥: {link}")
    print(f"å›¾ç‰‡: {image_url}")
    print(f"ä»·æ ¼: {price}")
    print("-" * 50)
```

### XPath (lxml)

```python
from lxml import html
import requests

response = requests.get('https://example.com')
tree = html.fromstring(response.content)

# XPathè¡¨è¾¾å¼
titles = tree.xpath('//h2[@class="title"]/text()')
links = tree.xpath('//a/@href')
prices = tree.xpath('//span[@class="price"]/text()')

# ç»„åˆæå–
items = tree.xpath('//div[@class="item"]')
for item in items:
    title = item.xpath('.//h2/text()')[0]
    price = item.xpath('.//span[@class="price"]/text()')[0]
```

---

## å¹¶å‘ç¼–ç¨‹

### å¤šçº¿ç¨‹

```python
import threading
import requests
from concurrent.futures import ThreadPoolExecutor

def fetch_url(url):
    response = requests.get(url)
    return response.status_code

urls = ['https://example.com/page1', 'https://example.com/page2']

# ä½¿ç”¨çº¿ç¨‹æ± 
with ThreadPoolExecutor(max_workers=5) as executor:
    results = executor.map(fetch_url, urls)

for url, status in zip(urls, results):
    print(f"{url}: {status}")
```

### å¼‚æ­¥IO (aiohttp)

```python
import aiohttp
import asyncio

async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text()

async def main():
    urls = ['https://example.com/page1', 'https://example.com/page2']
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return results

# è¿è¡Œ
results = asyncio.run(main())
```

---

## SeleniumåŠ¨æ€é¡µé¢

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

# é…ç½®Chromeé€‰é¡¹
chrome_options = Options()
chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('--no-sandbox')

# å¯åŠ¨æµè§ˆå™¨
driver = webdriver.Chrome(options=chrome_options)

try:
    # è®¿é—®é¡µé¢
    driver.get('https://example.com')
    
    # ç­‰å¾…å…ƒç´ åŠ è½½
    wait = WebDriverWait(driver, 10)
    element = wait.until(
        EC.presence_of_element_located((By.ID, "content"))
    )
    
    # æŸ¥æ‰¾å…ƒç´ 
    title = driver.find_element(By.TAG_NAME, 'h1').text
    items = driver.find_elements(By.CLASS_NAME, 'item')
    
    # ç‚¹å‡»æ“ä½œ
    button = driver.find_element(By.ID, 'load-more')
    button.click()
    
    # æ»šåŠ¨é¡µé¢
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    
    # è·å–é¡µé¢æºç 
    html = driver.page_source
    
finally:
    driver.quit()
```

---

## ååçˆ¬ç­–ç•¥

### 1. è¯·æ±‚å¤´ä¼ªè£…

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9',
    'Referer': 'https://www.example.com/',
    'Connection': 'keep-alive',
}
```

### 2. IPä»£ç†æ± 

```python
import random

proxy_pool = [
    'http://user:pass@host1:port',
    'http://user:pass@host2:port',
]

proxy = random.choice(proxy_pool)
response = requests.get(url, proxies={'http': proxy, 'https': proxy})
```

### 3. è¯·æ±‚é—´éš”

```python
import time
import random

time.sleep(random.uniform(1, 3))  # éšæœºå»¶æ—¶1-3ç§’
```

### 4. Cookieå’ŒSession

```python
session = requests.Session()
session.headers.update(headers)

# å…ˆè®¿é—®é¦–é¡µè·å–cookie
session.get('https://example.com')

# å†è®¿é—®ç›®æ ‡é¡µé¢
response = session.get('https://example.com/data')
```

---

## æ•°æ®å­˜å‚¨

```python
import json
import csv
import sqlite3

# JSON
with open('data.json', 'w', encoding='utf-8') as f:
    json.dump(data_list, f, ensure_ascii=False, indent=2)

# CSV
with open('data.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=['name', 'price'])
    writer.writeheader()
    writer.writerows(data_list)

# SQLite
conn = sqlite3.connect('data.db')
cursor = conn.cursor()
cursor.execute('''CREATE TABLE IF NOT EXISTS products 
                  (id INTEGER PRIMARY KEY, name TEXT, price REAL)''')
cursor.executemany('INSERT INTO products (name, price) VALUES (?, ?)', 
                   [(item['name'], item['price']) for item in data_list])
conn.commit()
conn.close()
```

---

## Scrapyæ¡†æ¶

### å¿«é€Ÿå¼€å§‹

```bash
pip install scrapy
scrapy startproject myproject
cd myproject
scrapy genspider example example.com
```

### Spiderç¤ºä¾‹

```python
# myproject/spiders/example_spider.py
import scrapy
from myproject.items import ProductItem

class ExampleSpider(scrapy.Spider):
    name = 'example'
    allowed_domains = ['example.com']
    start_urls = ['https://example.com/products']
    
    custom_settings = {
        'USER_AGENT': 'Mozilla/5.0...',
        'DOWNLOAD_DELAY': 1,
    }
    
    def parse(self, response):
        # æå–äº§å“åˆ—è¡¨
        products = response.css('.product-item')
        
        for product in products:
            item = ProductItem()
            item['name'] = product.css('.name::text').get()
            item['price'] = product.css('.price::text').get()
            
            # è·å–è¯¦æƒ…é¡µ
            detail_url = product.css('a::attr(href)').get()
            yield response.follow(detail_url, self.parse_detail, meta={'item': item})
        
        # ç¿»é¡µ
        next_page = response.css('.next-page::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_detail(self, response):
        item = response.meta['item']
        item['description'] = response.css('.description::text').get()
        yield item
```

---

## ğŸ¯ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹: è±†ç“£ç”µå½±Top250çˆ¬è™«

```python
import requests
from bs4 import BeautifulSoup
import json

class DoubanMovieSpider:
    def __init__(self):
        self.base_url = 'https://movie.douban.com/top250'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def get_page(self, offset):
        params = {'start': offset, 'limit': 25}
        response = requests.get(self.base_url, headers=self.headers, params=params)
        return response.text
    
    def parse_page(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        items = soup.find_all('div', class_='item')
        
        movies = []
        for item in items:
            movie = {
                'rank': item.find('em').text,
                'title': item.find('span', class_='title').text,
                'rating': item.find('span', class_='rating_num').text,
                'quote': item.find('span', class_='inq').text if item.find('span', class_='inq') else '',
            }
            movies.append(movie)
        
        return movies
    
    def crawl(self):
        all_movies = []
        for offset in range(0, 250, 25):
            print(f'çˆ¬å–ç¬¬{offset//25 + 1}é¡µ...')
            html = self.get_page(offset)
            movies = self.parse_page(html)
            all_movies.extend(movies)
        
        # ä¿å­˜æ•°æ®
        with open('douban_movies.json', 'w', encoding='utf-8') as f:
            json.dump(all_movies, f, ensure_ascii=False, indent=2)
        
        print(f'å…±çˆ¬å–{len(all_movies)}éƒ¨ç”µå½±')

# è¿è¡Œ
if __name__ == '__main__':
    spider = DoubanMovieSpider()
    spider.crawl()
```

---

## ğŸ“ é‡ç‚¹æ€»ç»“

### çˆ¬è™«å¼€å‘æµç¨‹

1. **åˆ†æç›®æ ‡ç½‘ç«™**: æŸ¥çœ‹robots.txtã€åˆ†æé¡µé¢ç»“æ„
2. **å‘é€è¯·æ±‚**: ä½¿ç”¨requestsæˆ–aiohttp
3. **è§£ææ•°æ®**: BeautifulSoupã€XPathã€æ­£åˆ™
4. **å¤„ç†åŠ¨æ€å†…å®¹**: Seleniumã€Playwright
5. **æ•°æ®å­˜å‚¨**: JSONã€CSVã€æ•°æ®åº“
6. **æ·»åŠ ååçˆ¬**: ä»£ç†ã€User-Agentã€å»¶æ—¶

### æ³¨æ„äº‹é¡¹

- éµå®ˆrobots.txtè§„åˆ™
- æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œä¸è¦å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›
- å¤„ç†å¼‚å¸¸å’Œé‡è¯•æœºåˆ¶
- ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è¯·æ±‚

---

**ä¸‹ä¸€æ­¥**: [[Pythonæ•°æ®åˆ†æ]] â†’ å­¦ä¹ NumPyå’ŒPandas
